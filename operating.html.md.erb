---
title: Operating an On-Demand Broker
owner: London Services Enablement
---

This topic provides information about operating the on-demand broker for
Pivotal Cloud Foundry (PCF) Ops Manager operators and BOSH operators.

## <a id="operator-checklist"></a> Operator Responsibilities

The operator is responsible for the following:

* Requesting appropriate networking rules for on-demand service tiles.
  See [Set Up Networking](#networking) below.

* Configuring the BOSH Director.
  See [Configure Your BOSH Director](#configure-bosh) below.

* Uploading the required releases for the broker deployment and service instance deployments.
  See [Upload Required Releases](#upload-releases) below.

* Writing a broker manifest.
  See [Write a Broker Manifest](#broker-manifest) below.

* Managing brokers and service plans.
  See [Broker and Service Management](./management.html).

<p class="note"><strong>Note</strong>: Pivotal recommends that you provide documentation
 when you make changes to the manifest to inform other operators about the new configurations.</p>

For a list of deliverables provided by the service author, see
[Service Author Deliverables](./creating.html#what-is-required-of-the-service-authors).

<!-- Partials are in https://github.com/pivotal-cf/docs-services-partials -->

## <a id="networking"></a>Set Up Networking

<%= partial '../../../svc-sdk/odb/partials/services/service_networks_table' %>

<br>
Regardless of the specific network layout, the operator must ensure network
rules are set up so that connections are open as described in the table below.

<table class="nice">
  <th>This component...</th>
  <th>Must communicate with...</th>
  <th>Default TCP Port</th>
  <th>Communication directions</th>
  <th>Notes</th>
  <tr>
    <td><strong>ODB</strong></td>
    <td>
        <ul>
            <li><strong>BOSH Director</strong></li>
          <li><strong>BOSH UAA</strong></li>
        </ul>
    </td>
    <td>
      <ul>
        <li>25555</li>
        <li>8443</li>
      </ul>
    </td>
    <td>One-way</td>
    <td>The default ports are not configurable.</td>
  </tr>
  <tr>
    <td><strong>ODB</strong></td>
    <td><strong>Deployed service instances</strong>
    </td>
    <td>Specific to the service (such as RabbitMQ for PCF).
      May be one or more ports.</td>
    <td>One-way</td>
    <td>This connection is for administrative tasks.
      Avoid opening general use, app-specific ports for this connection.</td>
  </tr>
  <tr>
    <td><strong>ODB</strong></td>
    <td><strong>PAS</strong>
    </td>
    <td>8443</td>
    <td>One-way</td>
    <td>The default port is not configurable.</td>
  </tr>
  <tr>
    <td><strong>Errand VMs</strong></td>
    <td>
      <ul>
        <li><strong>PAS</strong></li>
        <li><strong>ODB</strong></li>
        <li><strong>Deployed Service Instances</strong></li>
      </ul>
    </td>
    <td>
      <ul>
        <li>8443</li>
        <li>8080</li>
        <li>Specific to the service. May be one or more ports.</li>
      </ul>
    </td>
    <td>One-way</td>
    <td>The default port is not configurable.</td>
  </tr>
  <tr>
    <td><strong>BOSH Agent</strong></td>
    <td><strong>BOSH Director</strong>
    </td>
    <td>4222</td>
    <td>Two-way</td>
    <td>The BOSH Agent runs on every VM in the system, including the BOSH Director VM.
      The BOSH Agent initiates the connection with the BOSH Director.<br>
      The default port is not configurable.  </td>
  </tr>
  <tr>
    <td><strong>Deployed apps on PAS</strong></td>
    <td><strong>Deployed service instances</strong>
    </td>
    <td>Specific to the service. May be one or more ports.</td>
    <td>One-way</td>
    <td>This connection is for general use, app-specific tasks.
      Avoid opening administrative ports for this connection.</td>
  </tr>
  <tr>
    <td><strong>PAS</strong></td>
    <td><strong>ODB</strong>
    </td>
    <td>8080</td>
    <td>One-way</td>
    <td>This port may be different for individual services.
      This port may also be configurable by the operator if allowed by the
      tile developer.</td>
  </tr>
</table>

## <a id="configure-bosh"></a>Configure Your BOSH Director

See the following topics for how to set up your BOSH Director:

* [Software Requirements](#software-reqs)
* [SSL Certificates](#ssl-certs)
* [BOSH Teams](#bosh-teams)
* [Cloud Controller](#cloud-controller)

### <a id="software-reqs"></a> Software Requirements

The On-Demand Broker requires the following:

- BOSH Director v266.12.0 or v267.6.0 and later.
  To install the BOSH Director, see [Quick Start](https://bosh.io/docs/quick-start/)
  in the Cloud Foundry BOSH documentation.
- cf-release v1.10.0 or later (PCF v2.0 or later).

<div class="note">
  <strong>Notes: </strong>
  <ul>
    <li>ODB does not support BOSH Windows.</li>
    <li>Service instance lifecycle errands require BOSH Director v261 on PCF v1.10 or later. For more information, see <a href="#lifecycle-errands"> Service Instance Lifecycle Errands</a> below.</li>
  </ul>
</div>


### <a id="ssl-certs"></a>Configure SSL Certificates

If ODB is configured to communicate with BOSH on the Director's public IP address, you may
be using a self-signed certificate unless you have a domain for your BOSH Director.
ODB does not ignore TLS certificate validation errors by default.

You have two options to configure certificate-based authentication between the BOSH Director and the ODB:

* Add the BOSH Director's root certificate to ODB's trusted pool in the ODB manifest as follows:

    ```yaml
    bosh:
      root_ca_cert: ROOT-CA-CERT
    ```

* Use BOSH's `trusted_certs` feature to add a self-signed CA certificate to each VM that BOSH deploys.
For how to generate and use self-signed certificates for the BOSH Director and UAA,
see [Configuring SSL Certificates](https://bosh.io/docs/director-certs.html) in the BOSH documentation.

Optionally, you can configure a separate root CA certificate that is used when ODB communicates with the Cloud Foundry API (Cloud Controller).
For an example of how to add this in the manifest, see the line containing
`CA-CERT-FOR-CLOUD-CONTROLLER` in the manifest snippet in [Configure Your Manifest](#core-broker-config) below.


### <a id="bosh-teams"></a>Use BOSH Teams

You can use BOSH teams to further control how BOSH operations are available to
different clients. For more information about using BOSH teams,
see [Using Teams](https://bosh.io/docs/director-bosh-teams/)
in the BOSH documentation.


To use BOSH teams to ensure that your on-demand service broker client can only modify deployments it
created, do the following:

1. Run the following UAA CLI (UAAC) command to create the client:

    ```
    uaac client add CLIENT-ID \
      --secret CLIENT-SECRET \
      --authorized_grant_types "refresh_token password client_credentials" \
      --authorities "bosh.teams.TEAM-NAME.admin"
    ```
    Where:
    <ul>
      <li>`CLIENT-ID` is your client ID.</li>
      <li>`CLIENT-ID` is your client secret.</li>
      <li>`TEAM-NAME` is the name of the team authorized to modify this deployment.</li>
    </ul>
    <br>
    For example:
    <pre class="terminal">
    uaac client add admin \
      --secret 12345679 \
      --authorized\_grant\_types "refresh\_token password client\_credentials" \
      --authorities "bosh.teams.my-team.admin"
    </pre>

    For more information about using the UAAC, see [Creating and Managing Users
    with the UAA CLI (UAAC)](https://docs.cloudfoundry.org/uaa/uaa-user-management.html).

1. Configure the broker's BOSH authentication.
<br><br>
For example:

    ```yaml
    instance_groups:
      - name: broker
        ...
        jobs:
          - name: broker
            ...
            properties:
              ...
              bosh:
                url: DIRECTOR-URL
                root_ca_cert: CA-CERT-FOR-BOSH-DIRECTOR # optional, see SSL certificates
                authentication:
                  uaa:
                    client_id: BOSH-CLIENT-ID
                    client_secret: BOSH-CLIENT-SECRET
    ```
Where the `BOSH-CLIENT-ID` and `BOSH-CLIENT-SECRET` are the `CLIENT-ID` and `CLIENT-SECRET`
you provided in step 1.
<br><br>
The broker can then only perform BOSH operations on deployments it has created.
For a more detailed manifest snippet, see [Configure Your Manifest](#core-broker-config) below.


For more information about securing how ODB uses BOSH, see [Security](./security.html).

### <a id="cloud-controller"></a>Set Up Cloud Controller
<%# This "Set Up Cloud Controller" section needs to be improved for technical accuracy.  %>

ODB uses the Cloud Controller as a source of truth about service offerings, plans, and instances.

To reach the Cloud Controller, configure ODB with either client or user credentials
in the broker manifest. For more information, see [Write a Broker Manifest](#broker-manifest) below.


The following is an example broker manifest snippet for the client credentials:

```
authentication:
  ...
  client_credentials:
    client_id: UAA-CLIENT-ID # with cloud_controller.admin authority and client_credentials in the authorized_grant_type
    secret: UAA-CLIENT-SECRET
```

The following is an example broker manifest snippet for the user credentials:

```
authentication:
  ...
  user_credentials:
    username: CF-ADMIN-USERNAME # in the cloud_controller.admin and scim.read groups
    password: CF-ADMIN-PASSWORD
```


<div class="note">
  <p style="margin-top: 0"><strong>Note: </strong> The client or user must have the following permissions.</p>
  <ul>
    <li><strong>If using client credentials</strong>: As of Cloud Foundry v238, the UAA client must
  have the authority <code>cloud_controller.admin</code>.</li>
    <li><strong>If using user credentials</strong>: The user must be a Cloud Foundry admin user, that
  is, a member of the <code>scim.read</code> and <code>cloud_controller.admin</code> groups
  as a minimum.</li>
  </ul>
</div>

## <a id="upload-releases"></a>Upload Required Releases

Upload the following releases to your BOSH Director:

* **On Demand Service Broker (ODB)**---Download ODB from [Pivotal Network](https://network.pivotal.io/products/on-demand-services-sdk/).
* **Your service adapter**---Get the service adapter from the release author.
* **Your service release**---Get the service release from the release author.

To upload a release to your BOSH Director, do the following:

1. Run the following command.

    ```
    bosh -e BOSH-DIRECTOR-NAME upload-release RELEASE-FILE-NAME.tgz
    ```

    **Example command for ODB:**

    <pre class="terminal">
    $ bosh -e lite upload-release on-demand-service-broker-0.22.0.tgz
    </pre>

    <br>**Example commands for service adapter or service release:**
    <pre class="terminal">
    $ bosh -e lite upload-release my-service-release.tgz
    </pre>

    <pre class="terminal">
    $ bosh -e lite upload-release my-service-adapter.tgz
    </pre>

## <a id="broker-manifest"></a>Write a Broker Manifest

There are two parts to writing your broker manifest:

* [Configure Your Manifest](#core-broker-config)
* [Configure the Service Catalog and Plan Composition](#catalog)

If you are unfamiliar with writing BOSH v2 manifests, see [Manifest v2 Schema](http://bosh.io/docs/manifest-v2.html).

For example manifests, see the following:

  * For a Redis service---[redis-example-service-adapter-release](https://github.com/pivotal-cf-experimental/redis-example-service-adapter-release/blob/master/docs/example-manifest.yml).

  * For a Kafka service---[kafka-example-service-adapter-release](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml).

<p class="note"><strong>Note:</strong>
  If you are using a Xenial stemcell, you must update any BOSH add-ons to support Xenial stemcells.
  See <a href="./upgrades.html#update-addons">Update Add-ons to Run with Xenial Stemcell</a>.</p>

### <a id="core-broker-config"></a> Configure Your Manifest

Your manifest must contain one non-errand instance group that is colocated with
both of the following:

* The broker job from `on-demand-service-broker`
* Your service adapter job from your service adapter release

The broker is stateless and does not need a persistent disk.
The VM type can be small: a single CPU and 1 GB of memory is sufficient in most cases.

The snippet below uses the BOSH v2 syntax as well as global cloud
config and job-level properties.

<p class="note warning"><strong>WARNING</strong>: The <code>disable_ssl_cert_verification</code>
  option is dangerous and should be set to <code>false</code> in production</strong>.</p>

```yaml
instance_groups:
  - name: broker # this can be anything
    instances: 1
    vm_type: VM-TYPE
    stemcell: STEMCELL
    networks:
      - name: NETWORK
    jobs:
      - name: SERVICE-ADAPTER-JOB-NAME
        release: SERVICE-ADAPTER-RELEASE
      - name: broker
        release: on-demand-service-broker
        properties:
          # choose a port and basic auth credentials for the broker
          port: BROKER-PORT
          username: BROKER-USERNAME
          password: BROKER-PASSWORD
          disable_ssl_cert_verification: true|false # optional - defaults to false. This should NOT set to true in production
          shutdown_timeout_in_seconds: 60 # optional - defaults to 60 seconds. This allows the broker to gracefully wait for any open requests to complete before shutting down.
          expose_operational_errors: true|false # optional - defaults to false. This allows for BOSH operational errors to be displayed for the CF user.
          enable_plan_schemas: true|false # optional - defaults to false. If set to true, plan schemas are included in the catalog, and the broker fails if the adapter does not implement generate-plan-schemas.
          cf:
            url: CF-API-URL
            root_ca_cert: CA-CERT-FOR-CLOUD-CONTROLLER # optional - see SSL certificates
            authentication: # either client_credentials or user_credentials, not both as shown
              url: CF-UAA-URL
              client_credentials:
                client_id: UAA-CLIENT-ID # with cloud_controller.admin authority and client_credentials in the authorized_grant_type
                secret: UAA-CLIENT-SECRET
              user_credentials:
                username: CF-ADMIN-USERNAME # in the cloud_controller.admin and scim.read groups
                password: CF-ADMIN-PASSWORD
          bosh:
            url: DIRECTOR-URL
            root_ca_cert: CA-CERT-FOR-BOSH-DIRECTOR # optional - see SSL certificates
            authentication: # either basic or uaa, not both as shown
              basic:
                username: BOSH-USERNAME
                password: BOSH-PASSWORD
              uaa:
                client_id: BOSH-CLIENT-ID
                client_secret: BOSH-CLIENT-SECRET
          service_adapter:
            path: PATH-TO-SERVICE-ADAPTER-BINARY # optional - provided by the Service Author. Defaults to /var/vcap/packages/odb-service-adapter/bin/service-adapter

          # There are more broker properties that are discussed below
```


### <a id="catalog"></a>Configure Your Service Catalog and Plan Composition

Use the following sections as a guide to configure the service catalog and compose
plans in the properties section of broker job.
For an example snippet, see the [Starter Snippet for the Service Catalog and Plans](#starter-snippet) below.

#### <a id="configure-catalog"></a> Configure the Service Catalog

When configuring the service catalog, supply the following:

* **The release jobs specified by the service author:**
  * Supply each release job exactly once.
  * You can include releases that provide many jobs, as long as each required job
  is provided by exactly one release.<br><br>

* **A stemcell:**
  <p class="note"><strong>Note:</strong>
  If you are using a Xenial stemcell, you must update any BOSH add-ons to support Xenial stemcells.
  For links to instructional topics about updating see <a href="./upgrades.html#update-addons">Update Add-ons to Run with Xenial Stemcell</a>.</p>
  * This is used on each VM in the service deployments.
  * Only supply one stemcell. ODB does not currently support service instance
  deployments that use a different stemcell for different instance groups.
  * Use exact versions for releases and stemcells. The use of `latest` and floating
  stemcells are not supported.<br><br>

* **Cloud Foundry service metadata for the service offering:**
  * This metadata is aggregated in the Marketplace and displayed in Apps Manager
  and the cf CLI.
  * You can use other arbitrary field names as needed in addition to the Open
  Service Broker API (OSBAPI) recommended fields.
  For information about the recommended fields for service metadata, see the
  [Open Service Broker API Profile](https://docs.pivotal.io/pivotalcf/services/catalog-metadata.html#services-metadata).

#### <a id="compose-plans"></a> Compose Plans

Service authors do not define plans, but instead expose plan properties.
Operators compose plans consisting of combinations of these properties, along with IaaS resources
and catalog metadata.

When composing plans, supply the following:

* **Cloud Foundry plan metadata for each plan:**<br><br>
  You can use other arbitrary field names in addition to the OSBAPI recommended fields.
  For information about the recommended fields for plan metadata,
  see the [Open Service Broker API Profile](https://github.com/openservicebrokerapi/servicebroker/blob/master/profile.md#plan-metadata-fields)
  on GitHub.<br><br>

* **Resource mapping:**
  * For each plan, supply resource mapping for each instance group that service authors specify.
  * The resource values must correspond to valid resource definitions in the BOSH
  Director's global cloud config.
  * Service authors might recommend resource configuration.
  For example, in single-node Redis deployments, an instance count greater than
  one does not make sense.
  Here, you can configure the deployment to span multiple availability zones (AZs).
  For how to do this, see [Availability Zones](https://bosh.io/docs/azs.html)
  in the BOSH documentation.
  * Service authors might provide errands for the service release.
  You can add an instance group of type `errand` by setting the `lifecycle` field.
  For an example, see `register-broker` in the [kafka-example-service-adapter-release](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/cb1597979eddc4482d4511d4402a2b3cf9dcfa9e/docs/example-manifest.yml#L160-L176)
  on GitHub.<br><br>

* **Values for plan properties:**
  * Plan properties are key-value pairs defined by the service authors.
  For example, including a boolean to enable disk persistence for Redis or a list
  of strings representing RabbitMQ plugins to load.
  * The service author should document whether a plan property:
      * Is mandatory or optional
      * Precludes the use of another
      * Affects recommended instance group to resource mappings
  * You can also specify global properties at the service offering level, where
  they are applied to every plan.
  If there is a conflict between global and plan-level properties, the plan
  properties take precedence.<br><br>

* **(Optional) Provide an update block for each plan**
  * You might require plan-specific configuration for BOSH's update instance operation.
  ODB passes the plan-specific update block to the service adapter.
  * Plan-specific update blocks should have the same structure as the update block in a BOSH manifest.
  See [Update Block](https://bosh.io/docs/manifest-v2.html#update) in the BOSH documentation.
  * The service author can define a default update block to be used when a plan-specific update block is not provided,
  if the service adapter supports configuring update blocks in the manifest.

* **(Optional) Maintenance Info**
  * Maintenance info is used to uniquely describe
   the deployed version of a service instance. It is made up of public and
   private key/value pairs defined at the catalog and plan levels. It is
   displayed in the service catalog at plan level with plan-level maintenance
   information aggregated into the catalog-level maintenance information. The public
   information is exposed in the service catalog, while all the private
   information is aggregated and hashed into a value representing those
   private values.
  * Maintenance information which is common to all plans should be defined at the
    service catalog level.
  * Plan-specific maintenance information should be defined at the plan level. Where
    a key is redefined at the plan level, it overrides the service catalog
    level value.
  * Pivotal recommends using YAML anchors and references to avoid repeating
    maintenance information values within the manifest.  For instance, the stemcell
    version can be anchors with the `&stemcellVersion` annotation, and then
    referenced in the maintenance information with the `*stemcellVersion` tag.


#### <a id="starter-snippet"></a>Starter Snippet for the Service Catalog and Plans

Append the snippet below to the properties section of the broker job that you
configured in _Configure Your Broker_.
Ensure that you provide the required information listed in
[Configure Your Service Catalog and Plan Composition](#catalog) above.

For examples of complete broker manifests, see [Write a Broker Manifest](#broker-manifest) above.

```yaml
service_deployment:
  releases:
    - name: SERVICE-RELEASE
      version: SERVICE-RELEASE-VERSION # Exact release version
      jobs: [RELEASE-JOBS-NEEDED-FOR-DEPLOYMENT-AND-LIFECYCLE-ERRANDS] # Service Author will specify list of jobs required
  stemcell: # every instance group in the service deployment has the same stemcell
    os: SERVICE-STEMCELL
    version: SERVICE-STEMCELL-VERSION # Exact stemcell version
service_catalog:
  id: CF-MARKETPLACE-ID
  service_name: CF-MARKETPLACE-SERVICE-OFFERING-NAME
  service_description: CF-MARKETPLACE-DESCRIPTION
  bindable: TRUE|FALSE
  plan_updatable: TRUE|FALSE # optional
  tags: [TAGS] # optional
  requires: [REQUIRED-PERMISSIONS] # optional
  dashboard_client: # optional
    id: DASHBOARD-OAUTH-CLIENT-ID
    secret: DASHBOARD-OAUTH-CLIENT-SECRET
    redirect_uri: DASHBOARD-OAUTH-REDIRECT-URI
  metadata: # optional
    display_name: DISPLAY-NAME
    image_url: IMAGE-URL
    long_description: LONG-DESCRIPTION
    provider_display_name: PROVIDER-DISPLAY-NAME
    documentation_url: DOCUMENTATION-URL
    support_url: SUPPORT-URL
  global_properties: {} # optional - applied to every plan.
  global_quotas: # optional
    service_instance_limit: INSTANCE-LIMIT # the maximum number of service instances across all plans
    resource_limits: # optional - resource usage limits, determined by the 'cost' of each service instance plan
      ips: RESOURCE-LIMIT
      memory: RESOURCE-LIMIT
  plans:
    - name: CF-MARKETPLACE-PLAN-NAME
      free: TRUE|FALSE # optional - used by the cf CLI to display whether this plan is "free" or "paid"
      plan_id: CF-MARKETPLACE-PLAN-ID
      description: CF-MARKETPLACE-DESCRIPTION
      cf_service_access: ENABLE|DISABLE|MANUAL # optional - enable by default.
      bindable: TRUE|FALSE # optional -  if specified, this takes precedence over the bindable attribute of the service
      metadata: # optional
        display_name: DISPLAY-NAME
        bullets: [BULLET1, BULLET2]
        costs:
          - amount:
              CURRENCY-CODE-STRING: CURRENCY-AMOUNT-FLOAT
            unit: FREQUENCY-OF-COST
      resource_costs: # optional – the 'cost' of each instance in terms of resource quotas
        memory: AMOUNT-OF-RESOURCE-IN-THIS-PLAN
      quotas: # optional
        service_instance_limit: INSTANCE-LIMIT # the maximum number of service instances for this plan
        resource_limits: # optional - resource usage limits for this plan
          memory: RESOURCE-LIMIT
      instance_groups: # resource mapping for the instance groups defined by the Service Author
        - name: SERVICE-AUTHOR-PROVIDED-INSTANCE-GROUP-NAME
          vm_type: VM-TYPE
          vm_extensions: [VM-EXTENSIONS] # optional
          instances: INSTANCE-COUNT
          networks: [NETWORK]
          azs: [AZ]
          persistent_disk_type: DISK # optional
        - name: SERVICE-AUTHOR-PROVIDED-LIFECYCLE-ERRAND-NAME # optional
          lifecycle: errand
          vm_type: VM-TYPE
          instances: INSTANCE-COUNT
          networks: [NETWORK]
          azs: [AZ]
      properties: {} # valid property key-value pairs are defined by the Service Author
      update: # optional
        canaries: 1 # optional
        max_in_flight: 2  # required
        canary_watch_time: 1000-30000 # required
        update_watch_time: 1000-30000 # required
        serial: true # optional
      lifecycle_errands: # optional
        post_deploy: # optional
          - name: ERRAND-NAME
            instances: [INSTANCE-NAME, ...] # optional - for co-locating errand
          - name: ANOTHER_ERRAND_NAME
        pre_delete: # optional
          - name: ERRAND-NAME
            instances: [INSTANCE-NAME, ...] # optional - for co-locating errand
```
## <a id="secure-bind-credentials"></a>(Optional) Access Manifest Secrets at Bind Time

<p class="note alert"><strong>Note</strong>: This feature does not work if you have configured <code>use_stdin</code> to be false.</p>

A service adapter might need to access secrets embedded in a service instance
manifest when processing a create binding request.
For example, it might need credentials with sufficient privileges to add a new
user to a service instance.
These credentials are in the service instance manifest.
ODB passes this manifest to the adapter in the `create-binding` call.

Secrets in the manifest can be:

- BOSH variables
- Literal BOSH CredHub references
- Plain text


If you use BOSH variables or literal CredHub references in your manifest,
do the following in the ODB manifest so that the service adapter can access the secrets:

1. Set the `enable_secure_manifests` flag to `true`.<br><br>
For example:

    ```
    instance_groups:
      - name: broker
        ...
        jobs:
          - name: broker
            ...
            properties:
              ...
              enable_secure_manifests: true
              ...
    ```

1.  Supply details for accessing the credentials stored in BOSH CredHub.
Replace the placeholder text below with your values for accessing CredHub:

    ```
    instance_groups:
      - name: broker
        ...
        jobs:
          - name: broker
            ...
            properties:
              ...
              enable_secure_manifests: true
              bosh_credhub_api:
                url: https://BOSH-CREDHUB-ADDRESS:8844/
                root_ca_cert: BOSH-CREDHUB-CA-CERT
                authentication:
                  uaa:
                    client_credentials:
                      client_id: BOSH-CREDHUB-CLIENT-ID
                      client_secret: BOSH-CREDHUB-CLIENT-SECRET
    ```

<br>
When the `enable_secure_manifests` flag is set to `true`, ODB queries
BOSH and its CredHub instance for secret values. ODB then generates a map of all
manifest variable names and CredHub references to secret values in the manifest.
ODB passes this map to the service adapter's `create-binding` call. For an
example of the JSON in the `create-binding` call, see the [Service
Adapter Interface Reference documentation](./adapter-reference.html#create-binding).

If ODB cannot resolve a particular secret, it logs the failure and omits
the unresolved secret from the passed secrets map. It is the responsibility of the adapter to
handle missing secrets based on whether they are required for binding creation.

<p class="note"><strong>Note:</strong> ODB will not fail if it cannot resolve a secret.</p>

## <a id="secure-binding"></a>(Optional) Enable Secure Binding
<p class="note alert"><strong>Note</strong>: This feature does not work if you have configured <code>use_stdin</code> to be false.</p>

If you enable secure binding, service instance credentials are stored securely in
runtime CredHub.
When users create bindings or service keys, ODB passes a secure reference to the
service credentials through the network instead of plain text.

### Requirements

To store service credentials in runtime CredHub, your deployment must meet the
following requirements:

- Be able to connect to CredHub v1.6.x or later.
This may be provided as part of your Cloud Foundry deployment.

- Your instance group requires access to the local DNS provider because the address
for CredHub is a local domain name.

<p class="note"><strong>Note: </strong>
  Pivotal recommends using BOSH DNS as a DNS provider.
  If you are using PCF v2.4 or later, you cannot use consul as a DNS provider
  because consul server VMs have been removed in Pivotal Application Service (PAS) v2.4 .</p>

### Procedure for Enabling Secure Binding

To enable secure binding, do the following:

1. Set up a new CredHub client in Cloud Foundry UAA with `credhub.write` and
`credhub.read` in its list of scopes.
For how to do this, see [Creating and Managing Users with the UAA CLI
(UAAC)](https://docs.cloudfoundry.org/uaa/uaa-user-management.html) in the Cloud Foundry documentation.

1. Update the broker job in the ODB manifest to consume the CredHub link.<br><br>
For example:

    ```
    instance_groups:
      - name: broker
        ...
        jobs:
          - name: broker
            consumes:
              credhub:
                from: credhub
                deployment: cf
    ```

1. Update the broker job in the ODB manifest to include the `secure_binding_credentials` section.
The CA certificate can be a reference to the certificate in the cf deployment
or inserted manually. <br><br>
For example:

    ```
    instance_groups:
      - name: broker
        ...
        jobs:
          - name: broker
            ...
            properties:
              ...
              secure_binding_credentials:
                enabled: true
                authentication:
                  uaa:
                    client_id: NEW-CREDHUB-CLIENT-ID
                    client_secret: NEW-CREDHUB-CLIENT-SECRET
                    ca_cert: ((cf.uaa.ca_cert))
      ```
      <br>
  Where `NEW-CREDHUB-CLIENT-ID` and `NEW-CREDHUB-CLIENT-SECRET` are the CredHub client
  credentials you created in step 1.

For a more detailed manifest snippet, see [Configure Your Manifest](#core-broker-config) above.

### How Credentials Are Stored on CredHub

The credentials for a given service binding are stored with the following format:

```
/C/:SERVICE-GUID/:SERVICE-INSTANCE-GUID/:BINDING-GUID/CREDENTIALS
```

The plain-text credentials are stored in CredHub under this key, and the key is
available under the `VCAP_SERVICES` environment variable for the app.


## <a id="plan-schemas"></a>(Optional) Enable Plan Schemas

As of OSBAPI Spec v2.13 ODB supports enabling plan schemas. For more information, see [OSBAPI Spec v2.13](https://github.com/openservicebrokerapi/servicebroker/blob/v2.13/spec.md#changes-since-v212) on GitHub.

When this feature is enabled, the broker validates incoming configuration parameters
against a schema during the provision, binding, and update of service instances.
The broker produces an error if the parameters do not conform.

To enable plan schemas, do the following:

1. Ensure that the service adapter implements the command `generate-plan-schemas`.
If it is not implemented, the broker will fail to deploy.
For more information about this command, see [generate-plan-schemas](./adapter-reference.html#generate-plan-schemas).

1. In the manifest, set the `enable_plan_schemas` flag to `true` as shown below.
   The default is `false`.

      ```yaml
      instance_groups:
        - name: broker
          ...
          jobs:
            - name: broker
              ...
              properties:
                ...
                enable_plan_schemas: true
      ```
For a more detailed manifest snippet, see [Configure Your Manifest](#core-broker-config) above.



## <a id="route"></a>(Optional) Register the Route to the Broker

You can register a route to the broker using the `route_registrar` job from the
routing release.
The `route_registrar` job achieves the following:

* Load balances multiple instances of ODB using Cloud Foundry router
* Allows access to ODB from the public internet

For more information, see [route_registrar job](http://bosh.io/jobs/route_registrar?source=github.com/cloudfoundry-incubator/cf-routing-release).

To register the route, colocate the `route_registrar` job with the `on-demand-service-broker`:

1. Download the routing release.
   See [cf-routing Release](http://bosh.io/releases/github.com/cloudfoundry-incubator/cf-routing-release?all=1).
1. Upload the routing release to your BOSH Director.
1. Add the `route_registrar` job to your deployment manifest and configure it with an HTTP route.
This creates a URI for your broker.
    <p class="note"><strong>Note:</strong>
    You must use the same port for the broker and the route. The broker defaults to 8080.</p>
  For how to configure the `route_registrar` job, see the
  [routing release](https://github.com/cloudfoundry/routing-release/blob/d59974071d97b9f1770dd170240bff2fe5ba1558/jobs/route_registrar/spec#L95-L100)
  GitHub repository.
1. If you configure a route, set the `broker_uri` property in the [register-broker errand](./management.html#register-broker).

## <a id="service-instance-quotas"></a>(Optional) Set Service Instance Quotas

Set service instance quotas to limit the number of service instances ODB can create.
You can set these quotas for service instances:

- **Global quotas**--limit the number of service instances across all plans.

- **Plan quotas**---limit the number of service instances for a given plan.

<p class="note"><strong>Note</strong>: These limits do not include orphaned deployments.
  See <a href="./troubleshooting-bosh.html#listing-orphans">List Orphan Deployments</a>
   and <a href="./management.html#orphan-deployments">Delete Orphaned Deployments</a>
   for more information.</p>

When creating a service instance, ODB checks the global service instance limit.
If this limit has not been reached, ODB checks the plan service instance limit.

### Procedure for Setting Service Instance Quotas

To set service instance quotas, do the following in the manifest:

1. Add a quotas section for the type of quota you want to use.<br><br>
  - **For global quotas** add `global_quotas` in the service catalog:

        ```yaml
        service_catalog:
          ...
          global_quotas:
            service_instance_limit: INSTANCE-LIMIT
            ...
        ```
        <br>
  - **For plan quotas** add `quotas` the plans you want to limit:

        ```yaml
        service_catalog:
          ...
          plans:
            - name: CF-MARKETPLACE-PLAN-NAME
              quotas:
                service_instance_limit: INSTANCE-LIMIT
        ```
        <br>
  Where `INSTANCE-LIMIT` is the maximum number of service instances allowed.

For a more detailed manifest snippet, see the [Starter Snippet](#starter-snippet) above.

## <a id="service-resource-quotas"></a>(Optional) Set Resource Quotas

Set resource quotas to limit resources, such as memory or disk, more effectively when combining plans that
consume different amounts of resources.
You can set these quotas for service resources:

- **Global resource quotas** -- limit how much of a certain resource is consumed
across all plans. ODB allows new instances to be created until their total
resources reach the global quota.
- **Plan resource quotas** -- limit how much of a certain resource is consumed by a
specific plan.

<p class="note"><strong>Note</strong>: These limits do not include orphaned deployments.
  See <a href="./troubleshooting-bosh.html#listing-orphans">List Orphan Deployments</a>
   and <a href="./management.html#orphan-deployments">Delete Orphaned Deployments</a>.</p>

When creating a service instance, ODB checks the global resource limit.
If this limit has not been reached, ODB checks the plan resource limit.

### Procedure for Setting Service Resource Quotas

To set resource quotas, do the following in the manifest:

1. Add a quotas section for the type of quota you want to use.

    ```
    quotas:
      resource_limits:
        RESOURCE-NAME: RESOURCE-LIMIT
    ```
  Where:<br>
  <ul>
    <li><code>RESOURCE-NAME</code> is a string defining the resource you want to limit.</li>
    <li><code>RESOURCE-LIMIT</code> is a value for the maximum allowed for each resource.</li>
  </ul>
For example:
    - **For global quotas** add `global_quotas` in the service catalog:

        ```yaml
        service_catalog:
          ...
          global_quotas:
            resource_limits:
              ips: 50
              memory: 150
        ```
        <br>
    - **For plan quotas** add `quotas` in the plans you want to limit:

        ```yaml
        service_catalog:
          ...
          plans:
            - name: my-plan
              quotas:
                resource_limits:
                  memory: 25
        ```

1. Add `resource_costs` in each plan to define the amount of resources your plan allocates
to each service instance.

    ```yaml
    resource_costs:
      RESOURCE-NAME: AMOUNT-OF-RESOURCE # the key is string-matched against keys in the global and plan-level resource quotas
    ```
    Where:<br>
    - `RESOURCE-NAME` is a string defining the resource you want to limit.
    - `AMOUNT-OF-RESOURCE` is the amount of the resource allocated to each
    service instance of this plan.

    For example:

    ```yaml
    service_catalog:
      ...
      plans:
        - name: my-plan
          resource_costs:
            memory: 5
    ```


For a more detailed manifest snippet, see the [Starter Snippet](#starter-snippet) above.

## <a id="broker-metrics"></a>(Optional) Configure Service Metrics

The ODB BOSH release contains a metrics job, which can be used to emit metrics
when colocated with [Pivotal Cloud Foundry Service Metrics SDK](https://network.pivotal.io/products/service-metrics-sdk/).
To do this, you must include the [Loggregator](https://github.com/cloudfoundry/loggregator) release.

Add the following jobs to the broker instance group:

```yaml
- name: service-metrics
  release: service-metrics
  properties:
    service_metrics:
      execution_interval_seconds: INTERVAL-BETWEEN-SUCCESSIVE-METRICS-COLLECTIONS
      origin: ORIGIN-TAG-FOR-METRICS
      monit_dependencies: [broker] # hardcode this
      ....snip....
      #Add Loggregator configuration here: see examples @ https://github.com/pivotal-cf/service-metrics-release/blob/master/manifests
      ....snip....
- name: service-metrics-adapter
  release: ODB-RELEASE
```

See an example of how the service metrics can be configured for an on-demand-broker
deployment in the [kafka-example-service-adapter-release](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml#L106)
manifest on GitHub.

Pivotal has tested this example configuration with Loggregator v58 and service-metrics v1.5.0.

For more information about service metrics, see [Service Metrics for Pivotal Cloud Foundry](http://docs.pivotal.io/service-metrics).

## <a id="binding-with-dns"></a>(Optional) Obtain BOSH DNS Addresses for Binding Creation

You can configure ODB to retrieve BOSH DNS addresses for service instances.
These addresses are passed to the service adapter when you create a binding.
This may be necessary when the binding for a service instance contains, or relies on,
BOSH DNS addresses for that deployment.

### Requirements

- A service that has this feature enabled in the service adapter<br>
For information for service authors about how to enable this feature for their on-demand
service, see [Enable ODB to Obtain BOSH DNS Addresses](./service-adapter.html#dns-addresses).
- BOSH Director v266.12 or v267.6 and later, available in Ops Manager v2.2.5 and later

### Procedure

To enable ODB to obtain BOSH DNS addresses for binding creation, do the following:

1. In the manifest, configure the `binding_with_dns` property on plans that require
DNS addresses to create a binding.
<br><br>
For more information about the properties to add, see [Options for _binding\_with
\_dns_](#binding-property) below.
<br><br>
For example:

    ```yaml
    service_catalog:
      ...
      plans:
        ...
        - name: plan-requiring-dns-addresses
          ...
          binding_with_dns:                 # add this section
            - name: leader-address
              link_provider: example-link-1
              instance_group: leader-node
            - name: follower-address
              link_provider: example-link-2
              instance_group: follower-node
              properties:
                azs: [z1, z2]
                status: healthy
    ```

    <br>
    Each entry in `binding_with_dns` is converted to a BOSH DNS address that is
    passed to the service adapter when you create a binding.

#### <a id="binding-property"></a>Options for _binding\_with\_dns_

The following table provides descriptions of the properties to add to the `binding_with_dns` section:

<table class="nice">
<col width="165">
<thead>
<tr>
<th>Property</th>
<th>Description</th>
<th>Mandatory/Optional</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>name</code></td>
<td>An arbitrary identifier used to identify the address when creating a binding</td>
<td>Mandatory</td>
</tr>
<tr>
<td><code>link_provider</code></td>
<td>This is the exposed name of the link. You can find this in the documentation for the service and under <code>provides.name</code> in the release <code>spec</code> file.<br><br> You can override it in the deployment manifest by setting the <code>as</code> property of the link.</td>
<td>Mandatory</td>
</tr>
<tr>
<td><code>instance_group</code></td>
<td>This is the name of the instance group sharing the link. The resultant DNS address resolves to IP addresses of this instance group.</td>
<td>Mandatory</td>
</tr>
<tr>
<td><code>properties.azs</code></td>
<td>This is a list of availability zone names. When this is provided, the resultant DNS address resolves to IP addresses in these zones.</td>
<td>Optional</td>
</tr>
<tr>
<td><code>properties.status</code></td>
<td>This is a filter for link address status (healthy, unhealthy, all, default). When this is provided, the resultant DNS address resolves to IP addresses with this status.</td>
<td>Optional</td>
</tr>
</tbody>
</table>

## <a id="startup-checks"></a>About Broker Startup Checks

The ODB does the following startup checks:

* It verifies that the CF and BOSH versions satisfy the minimum versions required.
If your service offering includes lifecycle errands, the minimum required version
for BOSH is higher.
For more information, see [Configure Your BOSH Director](#configure-bosh) above.
<br><br>
    If your system does not meet minimum requirements, you see an insufficient
    version error. For example:

    <pre class="terminal">
    CF API error: Cloud Foundry API version is insufficient, ODB requires CF v238+.
    </pre>

* It verifies that, for the service offering, no plan IDs have changed for plans
that have existing service instances.
If there are instances, you see the following error:
<pre class="terminal">
You cannot change the plan_id of a plan that has existing service instances.
</pre>

## <a id="broker-stop"></a>About Broker Shutdown

The broker tries to wait for any incomplete HTTPS requests to complete before shutting down.
This reduces the risk of leaving orphan deployments in the event that
the BOSH Director does not respond to the initial `bosh deploy` request.

You can determine how long the broker waits before being forced to shut down by
using the `broker.shutdown_timeout` property in the manifest.
The default is 60 seconds.
For more information, see <a href="#broker-manifest">Write a Broker Manifest</a> above.

## <a id="lifecycle-errands"></a>Service Instance Lifecycle Errands

 <p class="note"><strong>Note</strong>: This feature requires BOSH Director v261 or later.</p>

Service Instance lifecycle errands allow additional short-lived jobs to run as part of service instance deployment.
A deployment is only considered successful if all lifecycle errands exit successfully.

The service adapter must offer the errands as part of the service instance deployment.
For more information, see [Create a Service Release](./creating.html#create-a-service-release).

ODB supports the following lifecycle errands:

- `post_deploy` runs after creating or updating a service instance. An example use case is
  running a health check to ensure the service instance is functioning.<br>
  For more information about the workflow, see [Create or Update Service Instance with Post-Deploy Errands](./concepts.html#post-deploy).<br><br>
- `pre_delete` runs before the deletion of a service instance.
  An example use case is cleaning up data before a service shutdown. 
  For more information about the workflow, see [Delete a Service Instance with Pre-Delete Errands](./concepts.html#pre-delete).

### <a id="enable-errands"></a> Enable Service Instance Lifecycle Errands

Service instance lifecycle errands are configured on a per-plan basis.
Lifecycle errands do not run if you change a plan's lifecycle errand
configuration while an existing deployment is in progress.

To enable lifecycle errands, do the following steps.

1. Add each errand job in the following manifest places:
    - `service_deployment`
    - The plan's `lifecycle_errands` configuration
    - The plan's `instance_groups`

    Below is an example manifest snippet that configures lifecycle errands for a plan:

    ```yaml
    service_deployment:
      releases:
        - name: SERVICE-RELEASE
          version: SERVICE-RELEASE-VERSION
          jobs:
          - SERVICE-RELEASE-JOB
          - POST-DEPLOY-ERRAND-JOB
          - PRE-DELETE-ERRAND-JOB
          - ANOTHER-POST-DEPLOY-ERRAND-JOB
    service_catalog:
      plans:
        - name: CF-MARKETPLACE-PLAN-NAME
          lifecycle_errands:
            post_deploy:
              - name: POST-DEPLOY-ERRAND-JOB
              - name: ANOTHER-POST-DEPLOY-ERRAND-JOB
                disabled: true
            pre_delete:
              - name: PRE-DELETE-ERRAND-JOB
          instance_groups:
            - name: SERVICE-RELEASE-JOB
              ...
            - name: POST-DEPLOY-ERRAND-JOB
              lifecycle: errand
              vm_type: VM-TYPE
              instances: INSTANCE-COUNT
              networks: [NETWORK]
              azs: [AZ]
            - name: ANOTHER-POST-DEPLOY-ERRAND-JOB
              lifecycle: errand
              vm_type: VM-TYPE
              instances: INSTANCE-COUNT
              networks: [NETWORK]
              azs: [AZ]
            - name: PRE-DELETE-ERRAND-JOB
              lifecycle: errand
              vm_type: VM-TYPE
              instances: INSTANCE-COUNT
              networks: [NETWORK]
              azs: [AZ]
    ```

    Where `POST-DEPLOY-ERRAND-JOB` is the errand job you want to add.

### <a id="colocated-errands"></a> (Optional) Enable Colocated Errands

<p class="note"><strong>Note</strong>: This feature requires BOSH Director v263 or later.</p>

You can run both `post-deploy` and `pre-delete` errands as colocated errands.
Colocated errands run on an existing service instance group instead of a separate one.
This avoids additional resource allocation.

Like other lifecycle errands, colocated errands are deployed on a per-plan basis.
Currently the ODB supports colocating only the `post-deploy` or `pre-delete` errands.

For more information, see the [Errands](https://bosh.io/docs/errands.html) in the BOSH documentation.

To enable colocated errands for a plan, do the following steps.

1. Add each colocated errand job to the manifest as follows:
    - Add the errand in `service_deployment`.
    - Add the errand in the plan's `lifecycle_errands` configuration.
    - Set the instances the errand should run on in the `lifecycle_errands`.

    Below is an example manifest that includes a colocated post-deploy errand:

    ```yaml
    service_deployment:
      releases:
        - name: SERVICE-RELEASE
          version: SERVICE-RELEASE-VERSION
          jobs:
            - SERVICE-RELEASE-JOB
            - COLOCATED-POST-DEPLOY-ERRAND-JOB
    service_catalog:
      plans:
        - name: CF-MARKETPLACE-PLAN-NAME
          lifecycle_errands:
            post_deploy:
              - name: COLOCATED-POST-DEPLOY-ERRAND-JOB
                instances:
                  - SERVICE-RELEASE-JOB/0
              - name: NON-COLOCATED-POST-DEPLOY-ERRAND
          instance_groups:
            - name: NON-COLOCATED-POST-DEPLOY-ERRAND
              ...
            - name: SERVICE-RELEASE-JOB
              ...
    ```

    Where `COLOCATED-POST-DEPLOY-ERRAND-JOB` is the colocated errand you want to
    run and `SERVICE-RELEASE-JOB/0` is the instances you want the errand to run
    on.
